{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1TPuMyMW45q2tS2uF0aeUpgjM1G_qW_L1",
      "authorship_tag": "ABX9TyO80wQEJ9MVrAceNQeDh4jA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varad2001/Gender-classification-simple-project/blob/main/model_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M_QoavXfe9EA"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import keras \n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Conv2D, MaxPooling2D \n",
        "from keras.layers import Dense, Dropout \n",
        "from keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL import UnidentifiedImageError"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model from the corresponding directory\n",
        "!ls /content/drive/MyDrive/Colab_Notebooks/saved_model/my_model1\n",
        "new_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab_Notebooks/saved_model/my_model1')"
      ],
      "metadata": {
        "id": "qKvZFWvkfJM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e0ddcd-2861-49b5-8df4-e0b76ace1013"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\tkeras_metadata.pb  saved_model.pb  variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWLFNdR2f7vv",
        "outputId": "10f743e9-24a0-4df2-94cf-2bdeff950a30"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization (BatchN  (None, 50, 50, 1)        4         \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 50, 50, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 25, 25, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 25, 25, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 25, 25, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 25, 25, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 12, 12, 32)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4608)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               589952    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 599,910\n",
            "Trainable params: 599,844\n",
            "Non-trainable params: 66\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing:<br>\n",
        "1. Extract all images, convert them to nd-array and then add them to another nd-array.<br>\n",
        "2. Reshape the resulting nd-array and get predictions from the model.<br>\n",
        "3. Extract the probabilities for male or female label. <br>\n",
        "4. e.g. If predicting male images, extract the first elements of the predictions, count how many are greater than 0.6-males, how many are less than 0.4- females and remaining - confused. <br>\n",
        "5. Calculate the accuracy."
      ],
      "metadata": {
        "id": "KY1kFuVUGLHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for every image, a list containing two elements is returned: <br>\n",
        "first element : prob that the image belongs to male category<br>\n",
        "second element : prob that the image belongs to female category<br>\n",
        "e.g. <br>\n",
        "[0.8, 0.2] means that the model thinks that there is 80% chance that the given image is male. <br>"
      ],
      "metadata": {
        "id": "VjDilQtGK0rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we will extract the values of first elements for each image and compute the result as follows:<br>\n",
        "if the value > 0.6 ==> the prediction is 'male'<br>\n",
        "if the value < 0.4 ==> the prediction is 'female' <br>\n",
        "else the model is confused"
      ],
      "metadata": {
        "id": "Fw1IQ8x8L27u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the function which converts imgs into arrays and performs the necessary preprocessing\n",
        "# input : Image object , returns : 3-dim nd-array with shape(img_width, img_height, 1)\n",
        "\n",
        "def convert_img(img1):\n",
        "    img = img1.convert(\"L\")         # convert the image into greyscale\n",
        "    px = 50\n",
        "    img = img.resize((px,px))       # resize the image to 50*50 pixels, ==> total of 2500 pixels\n",
        "    img = np.asarray(img)           # convert the image into numpy array, ==> shape will be (50,50)\n",
        "    img = img / 255.0               # scale the pixel values between 0 to 1\n",
        "    img = img.reshape(px,px,1)      # convert the two dimensional array to three-dimensional array\n",
        "    return img"
      ],
      "metadata": {
        "id": "9KS3mUKS-ev0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ", UnidentifiedImageError\n",
        "# function to predict the category and calculate accuracy\n",
        "# input 'img' is given as array of shape (width, height, 3)\n",
        "# returns dictionary containing probabilities\n",
        "\n",
        "def predict_gender(img):\n",
        "    imgs = np.array([])\n",
        "    img = Image.fromarray(img)          # create image from input 'img' array\n",
        "    img = convert_img(img)              # perform preprocessing\n",
        "    imgs = np.append(imgs,img)          # store the array \n",
        "    imgs = imgs.reshape(1,50,50,1)      # input to the model must be provided as an array of shape : (no_of_imgs, px, px, 1)\n",
        "\n",
        "    prediction = new_model.predict(imgs)\n",
        "    male_prob = float(prediction[0][0])        # extract first element of prediction array which represents the prob of image being 'male' \n",
        "    female_prob = float(prediction[0][1])      # extract female prob\n",
        "    results = {'Male' : male_prob, 'Female' : female_prob}\n",
        "    if male_prob > 0.5:\n",
        "        print(\"Image category:Male\")\n",
        "    else :\n",
        "        print(\"Image category:Female\")\n",
        "    return results\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "PcTpeMo6dyaU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the path of the image\n",
        "img_file_path = '/content/Chris-Evans-buzz-cut-e1538974047133.jpg'"
      ],
      "metadata": {
        "id": "riolgc4KXY_V"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(img_file_path)\n",
        "img = np.asarray(img)\n",
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHT_a0Q_Wasj",
        "outputId": "0a240bbb-e99e-4700-a67d-9dc9831d7f5a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2171, 1401, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results  = predict_gender(img)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdidgofTWGcT",
        "outputId": "ab738d47-00fa-474c-c85c-de6ee3a4b1a7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image category:Male\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Female': 0.06721476465463638, 'Male': 0.932785153388977}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CrTWkjSPfL5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JRTQ3qwVeRwJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}