{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1TPuMyMW45q2tS2uF0aeUpgjM1G_qW_L1",
      "authorship_tag": "ABX9TyPdXkGV6Un1sClZsH8mwhwO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varad2001/Gender-classification-simple-project/blob/main/model_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M_QoavXfe9EA"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import keras \n",
        "from sklearn.model_selection import train_test_split \n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Conv2D, MaxPooling2D \n",
        "from keras.layers import Dense, Dropout \n",
        "from keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL import UnidentifiedImageError"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model from the corresponding directory\n",
        "!ls /content/drive/MyDrive/Colab_Notebooks/saved_model/my_model2\n",
        "new_model = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab_Notebooks/saved_model/my_model2\")"
      ],
      "metadata": {
        "id": "qKvZFWvkfJM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e35c70-6acd-4a3f-8f59-72b1fa00f1d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\tkeras_metadata.pb  saved_model.pb  variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWLFNdR2f7vv",
        "outputId": "70a0a48f-6e84-40b8-ac20-10b0bdd54051"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization (BatchN  (None, 50, 50, 1)        4         \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 50, 50, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 25, 25, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 25, 25, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 25, 25, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 25, 25, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 12, 12, 32)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4608)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               589952    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 599,910\n",
            "Trainable params: 599,844\n",
            "Non-trainable params: 66\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing:<br>\n",
        "1. Extract all images, convert them to nd-array and then add them to another nd-array.<br>\n",
        "2. Reshape the resulting nd-array and get predictions from the model.<br>\n",
        "3. Extract the probabilities for male or female label. <br>\n",
        "4. e.g. If predicting male images, extract the first elements of the predictions, count how many are greater than 0.6-males, how many are less than 0.4- females and remaining - confused. <br>\n",
        "5. Calculate the accuracy."
      ],
      "metadata": {
        "id": "KY1kFuVUGLHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for every image, a list containing two elements is returned: <br>\n",
        "first element : prob that the image belongs to male category<br>\n",
        "second element : prob that the image belongs to female category<br>\n",
        "e.g. <br>\n",
        "[0.8, 0.2] means that the model thinks that there is 80% chance that the given image is male. <br>"
      ],
      "metadata": {
        "id": "VjDilQtGK0rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we will extract the values of first elements for each image and compute the result as follows:<br>\n",
        "if the value > 0.6 ==> the prediction is 'male'<br>\n",
        "if the value < 0.4 ==> the prediction is 'female' <br>\n",
        "else the model is confused"
      ],
      "metadata": {
        "id": "Fw1IQ8x8L27u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the function which converts imgs into arrays\n",
        "def convert_img(img1):\n",
        "    img = img1.convert(\"L\")         # convert the image into greyscale\n",
        "    px = 50\n",
        "    img = img.resize((px,px))       # resize the image to 50*50 pixels, ==> total of 2500 pixels\n",
        "    img = np.asarray(img)           # convert the image into numpy array, ==> shape will be (50,50)\n",
        "    img = img / 255.0               # scale the pixel values between 0 to 1\n",
        "    img = img.reshape(px,px,1)      # convert the two dimensional array to one-dimensional array\n",
        "    return img"
      ],
      "metadata": {
        "id": "9KS3mUKS-ev0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ", UnidentifiedImageError\n",
        "# function to predict the category and calculate accuracy\n",
        "# takes input as file_path where image/s are stored ; returns dictionary containing results\n",
        "# dict is returned in following format : {<img_file>:<category(fe/male)>}\n",
        "\n",
        "def predict_gender(img_file_path):\n",
        "  # store the filenames of each image in the img_file_path directory\n",
        "  img_files = [img_file_path + '/' + img_file for img_file in os.listdir(img_file_path)]\n",
        "  \n",
        "  results = {}                   # store the results in the form of dict : {<file>:<male/female>}\n",
        "  males , females  = 0,0\n",
        "  \n",
        "  i=0\n",
        "  while i<len(img_files):        # for every image file : do following\n",
        "      img_file = img_files[i]\n",
        "      imgs = np.array([])\n",
        "      \n",
        "      try :\n",
        "        img = Image.open(img_file)        # open the image object\n",
        "      except Exception as e:              # the file is either not an image file or not supported\n",
        "        img_files.remove(img_file)        # remove that file from the list\n",
        "        continue\n",
        "      \n",
        "      img = convert_img(img)              # convert it to np-array\n",
        "      imgs = np.append(imgs,img)          # store the array \n",
        "      imgs = imgs.reshape(1,50,50,1)      # input to the model must be provided as an array of shape : (no_of_imgs, px, px, 1)\n",
        "      \n",
        "      prediction = new_model.predict(imgs)\n",
        "      male_prob = prediction[0][0]        # extract first element of prediction array which represents the prob of image being 'male' \n",
        "      if male_prob > 0.5:\n",
        "        males += 1\n",
        "        results[img_file]=\"male\"          # store that file with the label\n",
        "        print(\"Image category:Male\")\n",
        "      else :\n",
        "        females += 1\n",
        "        results[img_file]=\"female\"\n",
        "        print(\"Image category:Female\")\n",
        "      \n",
        "      i+=1\n",
        "\n",
        "  print(\"Male images:{} \\nFemales images:{}\".format(males,females))\n",
        "  return results\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "PcTpeMo6dyaU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the path where the image/s is/are located\n",
        "img_file_path = '/content/imgs'"
      ],
      "metadata": {
        "id": "riolgc4KXY_V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results  = predict_gender(img_file_path)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdidgofTWGcT",
        "outputId": "fd8775a9-7bc2-4f09-ce5e-0a5c4a05bbed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image category:Female\n",
            "Image category:Female\n",
            "Image category:Male\n",
            "Image category:Male\n",
            "Image category:Male\n",
            "Image category:Male\n",
            "Image category:Female\n",
            "Image category:Male\n",
            "Image category:Female\n",
            "Male images:5 \n",
            "Females images:4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/content/imgs/360_F_258899001_68CalsKTRk6PZQgWH9JhR4heBlncCko9.jpg': 'male',\n",
              " '/content/imgs/Chris-Evans-buzz-cut-e1538974047133.jpg': 'male',\n",
              " '/content/imgs/Rob-cc2d68e18be04acf90a74623c1087fd6.jpg': 'male',\n",
              " '/content/imgs/Stokes-Ageas-Squad.png': 'male',\n",
              " '/content/imgs/archetypal-female-_3249633c.jpg': 'female',\n",
              " '/content/imgs/ccd925bb9c3c0c5446f2b55c5597ee8c--nice-makeup-stunning-makeup.jpg': 'female',\n",
              " '/content/imgs/cook1.jpg': 'male',\n",
              " '/content/imgs/female-faces1.jpg': 'female',\n",
              " '/content/imgs/woman-6328478_960_720.jpg': 'female'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CrTWkjSPfL5x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cT5dgJCQeNO6"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}